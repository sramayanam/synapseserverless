{
	"name": "CosmoDBSynapseSparkBatchIngestion",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "mysparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "39657308-a63e-42a0-9d67-55f97c700726"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/a5dc76bb-fb9c-460f-bc90-a9dd011b12ca/resourceGroups/rg-analytics/providers/Microsoft.Synapse/workspaces/srramsynws/bigDataPools/mysparkpool",
				"name": "mysparkpool",
				"type": "Spark",
				"endpoint": "https://srramsynws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/mysparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"### Let's load the data into Spark DataFrames from the Storage Location."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dfStoreDemoGraphics = (spark\n",
					"                .read\n",
					"                .csv(\"/RetailData/StoreDemoGraphics.csv\", header=True, inferSchema='true')\n",
					"              )\n",
					"\n",
					"dfRetailSales = (spark\n",
					"                .read\n",
					"                .csv(\"/RetailData/RetailSales.csv\", header=True, inferSchema='true')\n",
					"              )\n",
					"\n",
					"dfProduct = (spark\n",
					"                .read\n",
					"                .csv(\"/RetailData/Products.csv\", header=True, inferSchema='true')\n",
					"              )"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Let's Create a spark database"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"CREATE DATABASE IF NOT EXISTS sreelakedb\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Load the data from Spark data Frame to Lake Database Tables"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dfRetailSales.write.mode(\"overwrite\").saveAsTable(\"sreelakedb.RetailSales\")"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dfStoreDemoGraphics.write.mode(\"overwrite\").saveAsTable(\"sreelakedb.StoreDemoGraphics\")"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dfProduct.write.mode(\"overwrite\").saveAsTable(\"sreelakedb.Products\")"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"source": [
					"### First create a Cosmos DB with RetailSalesDemoDB and create three collections called 1. Retail Sales 2. StoreDemographics 3. Products\n",
					"### Write the Spark Dataframe to the Azure Cosmos DB Collections\n",
					"\n",
					"#### The \"cosmos.oltp\" is the Spark format that enables connection to the Cosmos DB Transactional store.\n",
					"\n",
					"#### Also enable Synapse link on these three containers to read data from the analytical containers. The linked service shows three analytic containers ready to be queried as part of the lab"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dfRetailSales.write\\\n",
					"            .format(\"cosmos.oltp\")\\\n",
					"            .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
					"            .option(\"spark.cosmos.container\", \"RetailSales\")\\\n",
					"            .mode('append')\\\n",
					"            .save()\n",
					"\n",
					"dfStoreDemoGraphics.write\\\n",
					"            .format(\"cosmos.oltp\")\\\n",
					"            .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
					"            .option(\"spark.cosmos.container\", \"StoreDemoGraphics\")\\\n",
					"            .mode('append')\\\n",
					"            .save()\n",
					"\n",
					"\n",
					"dfProduct.write\\\n",
					"            .format(\"cosmos.oltp\")\\\n",
					"            .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
					"            .option(\"spark.cosmos.container\", \"Products\")\\\n",
					"            .mode('append')\\\n",
					"            .save()     \n",
					""
				],
				"execution_count": 8
			}
		]
	}
}