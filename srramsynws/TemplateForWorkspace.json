{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "srramsynws"
		},
		"RetailSalesDemoDB_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'RetailSalesDemoDB'"
		},
		"srramsynws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'srramsynws-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:srramsynws.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"srramsynws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://srramsynstorage.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/RetailSalesDemoDB')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('RetailSalesDemoDB_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/srramsynws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('srramsynws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/srramsynws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('srramsynws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CETAS')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE DataExplorationDB;\nGO\n\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'Lz8oq1dn';\n\nCREATE DATABASE SCOPED CREDENTIAL [WorkspaceIdentity] WITH IDENTITY = 'Managed Identity';\nGO\n\nCREATE EXTERNAL FILE FORMAT parquet_file_format\nWITH\n(  \n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n);\nGO\n\n\nCREATE EXTERNAL TABLE [dbo].taxideltatable WITH (\n        LOCATION = '/data/curatedparquet1/',\n        DATA_SOURCE = ContosoLake,\n        FILE_FORMAT = parquet_file_format\n) AS\nSELECT * FROM dbo.GreenTaxiCleansed;\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CreateView')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "DROP VIEW IF EXISTS GreenTaxiCleansed;\nGO\nCREATE VIEW GreenTaxiCleansed AS\nSELECT \nTOP 10000\nCASE WHEN t.VendorID = 2 THEN 'VeriFone Inc.' ELSE 'Creative Mobile Technologies' END as VendorName,\nYEAR(t.lpep_pickup_datetime) yr,\nMONTH(t.lpep_pickup_datetime) mo,\nDAY(t.lpep_pickup_datetime) pdy,\nDATEPART(HOUR,t.lpep_pickup_datetime) phr,\nDAY(t.lpep_dropoff_datetime) ddy,\nDATEPART(HOUR,t.lpep_dropoff_datetime) dhr,\nCASE \nWHEN t.RatecodeID = 1 THEN 'Standard rate' \nWHEN t.RatecodeID = 2 THEN 'JFK' \nWHEN t.RatecodeID = 3 THEN 'Newark' \nWHEN t.RatecodeID = 4 THEN 'Nassau or WestChester' \nWHEN t.RatecodeID = 5 THEN 'Negotiatied Fare' \nELSE 'Group Ride' END as RateCode,\nCASE \nWHEN t.payment_type = 1 THEN 'Credit Card' \nWHEN t.payment_type = 2 THEN 'Cash' \nWHEN t.payment_type = 3 THEN 'No charge' \nWHEN t.payment_type = 4 THEN 'Dispute' \nWHEN t.payment_type = 5 THEN 'Unkown' \nELSE 'Voided Trip' END as PaymentType,\nCASE WHEN t.trip_type=1 THEN 'Street-hail' ELSE 'Dispatch' END as TripType ,\nl.Borough as pickupborough, \nl.Zone as pickupzone, \nl.service_zone as pickupservicezone, \nl1.Borough as dropoffborough, \nl1.Zone dropoffzone, \nl1.service_zone dropoffservicezone,\nt.passenger_count,\nt.trip_distance ,\nt.fare_amount\t,\nt.extra ,\nt.mta_tax\t,\nt.tip_amount ,\nt.total_amount\nFROM\n    dbo.taxidata t\n    LEFT JOIN\n    dbo.location l\n    on t.PULocationID = l.LocationID\n    LEFT JOIN\n    dbo.location l1\n    on t.DOLocationID = l1.LocationID;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE EXTERNAL FILE FORMAT DeltaLakeFormat\nWITH (  \n    FORMAT_TYPE = DELTA\n);\nGO\n\n\nSELECT         \nMONTH(lpep_pickup_datetime) AS mo ,\npassenger_count,\nCOUNT(*) AS cnt\nFROM OPENROWSET(\n\n            BULK '/data/parquetfiles',\n            DATA_SOURCE = 'ContosoLake',\n            FORMAT = 'DELTA'\n    ) nyc\nWHERE MONTH(lpep_pickup_datetime) IN (8,9,10)\nGROUP BY\nMONTH(lpep_pickup_datetime),\npassenger_count",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LocalDatabaseUserAccess')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--CREATE a database scoped credential and run in the database u created\nCREATE DATABASE SCOPED CREDENTIAL [storage_credential] WITH IDENTITY='Managed Identity';\nGO\n--Create an external data source that uses the database scoped credential\nCREATE EXTERNAL DATA SOURCE ContosoLake1 WITH ( LOCATION = 'https://srramsynstorage.dfs.core.windows.net',CREDENTIAL = storage_credential);\nGO\n\n--You have to use the Credential created above in the external data source to access the storage or create an external table else the query fails\nSELECT\n    TOP 100 *,result.filepath(1)\nFROM\n    OPENROWSET(\n            BULK '/data/parquetfiles/green_tripdata_*.*',\n            DATA_SOURCE = 'ContosoLake1',\n            FORMAT='PARQUET'\n    ) AS [result];\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Paritioning')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE VIEW TaxiPartitioningView AS\nSELECT\nresult.*,result.filepath(1) as yearmonth\nFROM\n    OPENROWSET(\n            BULK '/data/parquetfiles/green_tripdata_*.*',\n            DATA_SOURCE='ContosoLake',\n            FORMAT='PARQUET'\n    ) AS [result];\n\nselect TOP 100 * from TaxiPartitioningView where yearmonth = '2022-11';",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/QueryingParquet')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT COUNT(*)\nFROM \nOPENROWSET(\nBULK 'https://azureopendatastorage.blob.core.windows.net/nyctlc/green/puYear=*/puMonth=*/*.parquet',\nFORMAT='PARQUET'\n) t\n\nCREATE DATABASE DataExplorationDB \n                COLLATE Latin1_General_100_BIN2_UTF8;\n\n\nUSE DataExplorationDB;\n\nCREATE DATABASE SCOPED CREDENTIAL [storage_credential] WITH IDENTITY='Managed Identity';\nGO\nCREATE EXTERNAL DATA SOURCE ContosoLake1 WITH ( LOCATION = 'https://srramsynstorage.dfs.core.windows.net',CREDENTIAL = storage_credential);\nGO\n\n\n\n\nGRANT ADMINISTER DATABASE BULK OPERATIONS TO data_explorer;\nGO\n\nALTER ROLE db_datareader\n\tADD MEMBER data_explorer;  \nGO\n\nCREATE EXTERNAL FILE FORMAT ParquetFormat\n    WITH (\n        FORMAT_TYPE = PARQUET\n    );\nGO\n\nSELECT\n    TOP 100 *,result.filepath(1)\nFROM\n    OPENROWSET(\n            BULK '/data/parquetfiles/green_tripdata_*.*',\n            DATA_SOURCE = 'ContosoLake',\n            FORMAT='PARQUET'\n    ) AS [result];\n\nDROP EXTERNAL TABLE dbo.taxidata;\nCREATE EXTERNAL TABLE dbo.taxidata\n(\nVendorID INT,\nlpep_pickup_datetime DATETIME2,\nlpep_dropoff_datetime DATETIME2,\nstore_and_fwd_flag\tVARCHAR(2),\nRatecodeID\tfloat,\nPULocationID INT,\nDOLocationID INT,\npassenger_count\tfloat,\ntrip_distance float,\nfare_amount\tfloat,\nextra float,\nmta_tax\tfloat,\ntip_amount float,\ntolls_amount float,\nehail_fee VARCHAR(10),\nimprovement_surcharge float,\ntotal_amount float,\npayment_type float,\t\ntrip_type float,\ncongestion_surcharge float\n)\nWITH\n(\n    LOCATION='data/parquetfiles/green_tripdata_*.parquet',\n    DATA_SOURCE=ContosoLake,\n    FILE_FORMAT=ParquetFormat\n);\nGO\n\nSELECT TOP 100 * FROM dbo.taxidata;\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'data/parquetfiles/green_tripdata_2022-08.parquet',\n        DATA_SOURCE='ContosoLake',\n        FORMAT='PARQUET'\n    ) AS [result];\n\n\n\n-- query the table\n\n\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://srramsynstorage.dfs.core.windows.net/data/parquetfiles/*.*',\n    FORMAT = 'parquet') AS rows\n\n\nDROP EXTERNAL TABLE dbo.taxidata1;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ReadJson')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "select\nid,\nCASE WHEN parent1 IS NULL THEN concat(parent1fname,' ',parent1lname) ELSE parent1 END Parent1Name,\nCASE WHEN parent2 IS NULL THEN concat(parent2fname,' ',parent2lname) ELSE parent2 END Parent2Name\nfrom openrowset(\n        bulk 'https://srramsynstorage.blob.core.windows.net/data/jsonl/families.jsonl',\n                format='csv', fieldterminator ='0x0b', fieldquote = '0x0b'\n    ) with (doc nvarchar(max)) as rows\ncross apply openjson (doc)\n        with (  \n                id varchar(100),\n                parent1 varchar(25) '$.parents[0].firstName',\n                parent2 varchar(25) '$.parents[1].firstName',\n                parent1fname varchar(25) '$.parents[0].familyName',\n                parent1lname varchar(25) '$.parents[0].givenName',\n                parent2fname varchar(25) '$.parents[1].familyName',\n                parent2lname varchar(25) '$.parents[1].givenName'\n                )",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ReadMasterData')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT\nTOP 100 *\nFROM\n    OPENROWSET(\n            BULK '/data/csvfiles/taxi_zone_lookup.csv',\n            DATA_SOURCE = 'ContosoLake',\n            FORMAT='csv',\n            PARSER_VERSION = '2.0'\n    ) AS [location];\n\nDROP EXTERNAL FILE FORMAT CsvFormat;\nCREATE EXTERNAL FILE FORMAT CsvFormat\n    WITH (\n        FORMAT_TYPE = DELIMITEDTEXT,\n        FORMAT_OPTIONS(\n            FIELD_TERMINATOR = ',',\n            STRING_DELIMITER = '\"',\n            FIRST_ROW = 2\n        )\n    );\nGO\n\nDROP EXTERNAL TABLE dbo.location;\nCREATE EXTERNAL TABLE dbo.location\n(\n         LocationID INT,\n         Borough VARCHAR(50),\n         Zone VARCHAR(50) COLLATE Latin1_General_100_BIN2_UTF8,\n         service_zone VARCHAR(50) COLLATE Latin1_General_100_BIN2_UTF8\n)\nWITH\n(\n    LOCATION='/data/csvfiles/taxi_zone_lookup.csv',\n    DATA_SOURCE=ContosoLake,\n    FILE_FORMAT=CsvFormat\n);\nGO\n\nSELECT TOP 100 * from  dbo.location\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SharedSparkTables')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT * FROM dbo.sharedsparkproducts ;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sreelakedb",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SynapseLink')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP 10 *\nFROM OPENROWSET(\n      PROVIDER = 'CosmosDB',\n      CONNECTION = 'Account=srramcosmosdb;Database=RetailSalesDemoDB',\n      OBJECT = 'RetailSales',\n      SERVER_CREDENTIAL = 'MyCosmosDbAccountCredential'\n    ) as rows",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/createusersadmin')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE LOGIN data_explorer WITH PASSWORD = 'Lz8oq1dn';\nCREATE LOGIN data_user WITH PASSWORD = 'Lz8oq1dn';\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'Lz8oq1dn';\nGRANT ALTER ANY CREDENTIAL TO data_user;\nCREATE DATABASE SCOPED CREDENTIAL [storage_credential] WITH IDENTITY='Managed Identity';\nGO\nGRANT REFERENCES ON CREDENTIAL::[storage_credential] TO data_user;\nGO\n\n\nCREATE CREDENTIAL MyCosmosDbAccountCredential\n    WITH IDENTITY = 'SHARED ACCESS SIGNATURE', SECRET = 'xw4e0bEjHVGK3AtgQlBtMUKzX3OZx1Wu7Dwdww96CVuM4k30lAnj5zjXNmqHvw6FpEisYh5zTfM2ACDbU69O9g==';\n\n\nGRANT CONNECT ANY DATABASE to [login@contoso.com];\nGRANT SELECT ALL USER SECURABLES to [login@contoso.com];\n\nCREATE USER data_user FOR LOGIN data_user;\nGO\n\nALTER ROLE db_owner ADD member data_user;\nGO\n\nCREATE DATABASE SCOPED CREDENTIAL SynapseIdentity WITH IDENTITY = 'Managed Identity';\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/statistics')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE STATISTICS stat_payment_type ON dbo.taxidata(payment_type);\n\nSELECT payment_type,count(*)\nFROM\ndbo.taxidata\ngroup by \npayment_type;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/1CosmoDBSynapseSparkBatchIngestion')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "mysparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "97303611-6116-4876-98db-c4cfb7c87354"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a5dc76bb-fb9c-460f-bc90-a9dd011b12ca/resourceGroups/rg-analytics/providers/Microsoft.Synapse/workspaces/srramsynws/bigDataPools/mysparkpool",
						"name": "mysparkpool",
						"type": "Spark",
						"endpoint": "https://srramsynws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/mysparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Batch Data Ingestion for Near real-time sales forecasting leveraging Synapse Link for Azure Cosmos DB\n",
							"\n",
							"## Key Information about this notebook\n",
							"\n",
							"* This notebook is part of the Azure Synapse Link for Azure Cosmos DB analitycal sample notebooks. For more information, click [here](../../../README.md). \n",
							"\n",
							"* It was build for Azure Cosmos DB SQL API but you can, by yourself, customize it for Azure Cosmos DB API for MongoDB. Please read about the analytical store inference schema differences between these 2 APIs [here](https://docs.microsoft.com/azure/cosmos-db/analytical-store-introduction#analytical-schema). \n",
							"\n",
							"* This is a Synapse Notebook and it was created to run in Synapse Analytics workspaces. Please make sure that you followed the pre-reqs of the [README](/README.md) file. After that, please execute the steps below in the same order that they are presented here. \n",
							"\n",
							"* From now on, all operations are case sentitive. Please be careful with everything you need to type.\n",
							"\n",
							"## Predictive Analytics\n",
							"\n",
							"Predictive analytics can help us to study and discover the factors that determine the number of sales that a retail store will have in the future. This notebook scenario is [Microsoft Surface](https://www.microsoft.com/en-us/surface) sales forecasting, with artificially created data. The business challenge is a **distributor that wants to predict how many units are necessary in the local warehouse to supply the stores in the area.**\n",
							"\n",
							"We will use Quantitative Models to forecast future data as a function of past data. They are appropriate to use when past numerical data is available and when it is reasonable to assume that some of the patterns in the data are expected to continue into the future. These methods are usually applied to short or intermediate range decisions. For more information, click [here](https://en.wikipedia.org/wiki/Forecasting).\n",
							"\n",
							"\n",
							"<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/store.PNG\" alt=\"Surface Device\" width=\"75%\"/>\n",
							"\n",
							"## Environment Creation\n",
							"\n",
							"### 1. Using the **Data / Linked** tab of your Synapse workspace, one of the pre-requisites mentioned in the README file, create a **RetailData** folder within the root directory of your storage account. Upload to this folder the csv files that are placed within the folder with the same name of this repo. As you can see in the image below, your new folder in the storage account, a data lake, will be in the second level of the file system structure. \n",
							"\n",
							"**Did you know?**  The Synapse workspace is attached to an [ADLS Gen2 storage account](https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction) and the files placed on the default storage account can be accessed using the relative path as below.\n",
							"\n",
							"<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/upload.PNG\" alt=\"Upload\" width=\"75%\"/>\n",
							"\n",
							"### 2. Using the Azure Portal, go to the **Access Control (IAM)** tab, click on the **+Add** and **Add a role assignment** links and add yourself to the **Contributor** role. This will allow you to create databases and tables within from your Azure Synapse Spark Pool.\n",
							"\n",
							"### 3. Using the Azure Portal, go to [Data Explorer](https://docs.microsoft.com/en-us/azure/cosmos-db/data-explorer) of your the Azure Cosmos DB Account and create a database called **RetailSalesDemoDB**. Change the Throughput to Autoscale and set the limit to 40,000 instead of 400, this will speed-up the loading process of the data, scaling down the database when it is not in use. For more information, click [here](https://docs.microsoft.com/en-us/azure/cosmos-db/provision-throughput-autoscale).\n",
							"\n",
							"### 4. In the same Data Explorer, create 3 **Analytical Store** enabled containers: **StoreDemoGraphics**, **RetailSales**, and **Products**. In the portal interface, the container-id is the container name. Important details:\n",
							"+ Use **/id** as the Partition key for all 3 containers.\n",
							"+ Please make sure that **Analytical store** is set to **On** for all 3 containers. \n",
							"\n",
							"\n",
							"<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/new-cont.PNG\" alt=\"New Containers\" width=\"75%\"/>\n",
							"\n",
							"\n",
							"### 5. In your Azure Synapse workspace, go to the **Manage / Linked Services** tab and create a service called **RetailSalesDemoDB** pointing to the database in Cosmos DB created in item 3 above.\n",
							"\n",
							"<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/ls.PNG\" alt=\"Surface Device\" width=\"100%\"/>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## "
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"### 6. Now let's load the data into Spark DataFrames."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfStoreDemoGraphics = (spark\n",
							"                .read\n",
							"                .csv(\"/RetailData/StoreDemoGraphics.csv\", header=True, inferSchema='true')\n",
							"              )\n",
							"\n",
							"dfRetailSales = (spark\n",
							"                .read\n",
							"                .csv(\"/RetailData/RetailSales.csv\", header=True, inferSchema='true')\n",
							"              )\n",
							"\n",
							"dfProduct = (spark\n",
							"                .read\n",
							"                .csv(\"/RetailData/Products.csv\", header=True, inferSchema='true')\n",
							"              )\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dfRetailSales.write\\\n",
							"            .format(\"cosmos.oltp\")\\\n",
							"            .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
							"            .option(\"spark.cosmos.container\", \"RetailSales\")\\\n",
							"            .mode('append')\\\n",
							"            .save()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"CREATE DATABASE IF NOT EXISTS sreelakedb\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dfRetailSales.write.mode(\"overwrite\").saveAsTable(\"sreelakedb.RetailSales\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dfStoreDemoGraphics.write.mode(\"overwrite\").saveAsTable(\"sreelakedb.StoreDemoGraphics\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dfProduct.write.mode(\"overwrite\").saveAsTable(\"sreelakedb.Products\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"### 7. Write the dataframe to the Azure Cosmos DB Collections\n",
							"\n",
							">**Did you know?** The \"cosmos.oltp\" is the Spark format that enables connection to the Cosmos DB Transactional store.\n",
							"\n",
							">**Did you know?** The ingestion to the Azure Cosmos DB collection is always performed through the Transactional store irrespective of whether the Analytical Store is enabled or not."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfStoreDemoGraphics.write\\\n",
							"            .format(\"cosmos.oltp\")\\\n",
							"            .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
							"            .option(\"spark.cosmos.container\", \"StoreDemoGraphics\")\\\n",
							"            .mode('append')\\\n",
							"            .save()\n",
							"\n",
							"\n",
							"dfProduct.write\\\n",
							"            .format(\"cosmos.oltp\")\\\n",
							"            .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
							"            .option(\"spark.cosmos.container\", \"Products\")\\\n",
							"            .mode('append')\\\n",
							"            .save()     \n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"### 8. Using the Azure Cosmos DB Account portal, go to the Data Explorer and check if the data was loaded.\n",
							"\n",
							"## All Done! Now let's go to the [Forecast Notebook](2SalesForecastingWithAML.ipynb)."
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CreateSampleDeltaLakeFiles')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "mysparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "e35ec587-db01-4ffc-9d4c-81b811ffa298"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a5dc76bb-fb9c-460f-bc90-a9dd011b12ca/resourceGroups/rg-analytics/providers/Microsoft.Synapse/workspaces/srramsynws/bigDataPools/mysparkpool",
						"name": "mysparkpool",
						"type": "Spark",
						"endpoint": "https://srramsynws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/mysparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": true
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"from delta.tables import DeltaTable\n",
							"deltaTable = DeltaTable.convertToDelta(spark, \"parquet.`abfss://data@srramsynstorage.dfs.core.windows.net/parquetfiles`\")"
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CreateServerlessTables')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "mysparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "5481c796-64c9-4a4f-b2a2-6a80893d90d9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a5dc76bb-fb9c-460f-bc90-a9dd011b12ca/resourceGroups/rg-analytics/providers/Microsoft.Synapse/workspaces/srramsynws/bigDataPools/mysparkpool",
						"name": "mysparkpool",
						"type": "Spark",
						"endpoint": "https://srramsynws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/mysparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"CREATE TABLE sreelakedb.sharedsparkproducts\n",
							"    USING Parquet\n",
							"    LOCATION \"abfss://data@srramsynstorage.dfs.core.windows.net/synapse/workspaces/srramsynws/warehouse/sreelakedb.db/products/\""
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mysparkpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 15,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "select\n    TOP 100\n    JSON_VALUE(doc, '$.date_rep') AS date_reported,\n    JSON_VALUE(doc, '$.countries_and_territories') AS country,\n    CAST(JSON_VALUE(doc, '$.deaths') AS INT) as fatal,\n    JSON_VALUE(doc, '$.cases') as cases,\n    doc\nfrom openrowset(\n        bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.jsonl',\n                format='csv', fieldterminator ='0x0b', fieldquote = '0x0b'\n    ) with (doc nvarchar(max)) as rows\norder by JSON_VALUE(doc, '$.geo_id') desc;\n\nselect\ndate_rep,\ncases,\nfatal,\ncountry\nfrom openrowset(\n        bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.jsonl',\n        format='csv', \n        fieldterminator ='0x0b', \n        fieldquote = '0x0b'\n\n    ) with (doc nvarchar(max)) as rows\n    cross apply openjson (doc)\n        with (  date_rep datetime2,\n                cases int,\n                fatal int '$.deaths',\n                country varchar(100) '$.countries_and_territories')\nwhere country = 'India'\norder by country, date_rep desc;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		}
	]
}